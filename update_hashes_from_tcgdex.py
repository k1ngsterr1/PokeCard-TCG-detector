#!/usr/bin/env python3
"""
Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ĞºĞ°Ñ€Ñ‚ Ğ¸Ğ· TCGdex API Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ñ…ĞµÑˆĞµĞ¹
Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ API Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ° ĞºĞ°Ñ€Ñ‚ Ğ¸ Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
"""
import requests
import pandas as pd
import imagehash
from PIL import Image
from io import BytesIO
import time
import pickle
import os

# API endpoints
TCGDEX_API = "https://api.tcgdex.net/v2/en"

def get_all_sets():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… ÑĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· TCGdex"""
    print("ğŸ“¦ Fetching all sets from TCGdex...")
    response = requests.get(f"{TCGDEX_API}/sets")
    if response.status_code == 200:
        sets = response.json()
        print(f"âœ… Found {len(sets)} sets")
        return sets
    else:
        print(f"âŒ Failed to fetch sets: {response.status_code}")
        return []

def get_cards_from_set(set_id):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ‚Ğ°"""
    response = requests.get(f"{TCGDEX_API}/sets/{set_id}")
    if response.status_code == 200:
        set_data = response.json()
        return set_data.get('cards', [])
    return []

def compute_hashes_for_image(image_url, card_id=None):
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ…ĞµÑˆĞ¸"""
    try:
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
        response = requests.get(image_url, timeout=15)
        if response.status_code != 200:
            return None, f"HTTP {response.status_code}"
        
        img = Image.open(BytesIO(response.content))
        
        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ…ĞµÑˆĞ¸
        phash = imagehash.phash(img, 32, 8)
        dhash = imagehash.dhash(img, 32)
        whash = imagehash.whash(img, 32)
        chash = imagehash.colorhash(img)
        
        return {
            'perceptual': phash,
            'difference': dhash,
            'wavelet': whash,
            'color': chash
        }, None
    except requests.exceptions.Timeout:
        return None, "Timeout (>15s)"
    except requests.exceptions.RequestException as e:
        return None, f"Network error: {str(e)[:30]}"
    except Exception as e:
        return None, f"Error: {str(e)[:30]}"

def update_hash_database(batch_size=50, start_from=0, limit=None):
    """
    ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ·Ñƒ Ñ…ĞµÑˆĞµĞ¹ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· TCGdex
    
    Args:
        batch_size: ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ° Ñ€Ğ°Ğ· Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼
        start_from: Ğ¡ ĞºĞ°ĞºĞ¾Ğ³Ğ¾ ÑĞµÑ‚Ğ° Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ (Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸)
        limit: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ (None = Ğ²ÑĞµ)
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ±Ğ°Ğ·Ñƒ
    if os.path.exists('card_hashes_32b.pickle'):
        print("ğŸ“š Loading existing hash database...")
        existing_df = pd.read_pickle('card_hashes_32b.pickle')
        existing_ids = set(existing_df['id'].values)
        print(f"âœ… Loaded {len(existing_ids)} existing cards")
    else:
        print("ğŸ†• Creating new hash database...")
        existing_df = pd.DataFrame(columns=['id', 'perceptual', 'difference', 'wavelet', 'color'])
        existing_ids = set()
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ ÑĞµÑ‚Ñ‹
    all_sets = get_all_sets()
    
    if not all_sets:
        print("âŒ No sets found. Exiting.")
        return
    
    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞµÑ‚Ñ‹ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ (Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ)
    all_sets = sorted(all_sets, key=lambda x: x.get('releaseDate', ''), reverse=True)
    
    new_hashes = []
    total_processed = 0
    total_added = 0
    total_skipped = 0
    
    print(f"\nğŸš€ Starting hash computation...")
    print(f"   Will process sets starting from index {start_from}")
    if limit:
        print(f"   Limit: {limit} cards")
    print()
    
    for set_idx, set_info in enumerate(all_sets[start_from:], start=start_from):
        set_id = set_info['id']
        set_name = set_info.get('name', set_id)
        
        print(f"\nğŸ“¦ [{set_idx + 1}/{len(all_sets)}] Processing set: {set_name} ({set_id})")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¸Ğ· ÑĞµÑ‚Ğ°
        cards = get_cards_from_set(set_id)
        print(f"   Found {len(cards)} cards in set")
        
        for card_idx, card in enumerate(cards, 1):
            card_id = card.get('id')
            card_name = card.get('name', 'Unknown')
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚
            if limit and total_processed >= limit:
                print(f"\nâœ… Reached limit of {limit} cards. Stopping.")
                break
            
            total_processed += 1
            
            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞµÑĞ»Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ° ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ
            if card_id in existing_ids:
                total_skipped += 1
                if card_idx % 10 == 0:
                    print(f"   [{card_idx}/{len(cards)}] Skipping existing cards... ({total_skipped} skipped)")
                continue
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ URL Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ high quality)
            image_url = None
            if 'image' in card:
                # image Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼
                if isinstance(card['image'], str):
                    image_url = card['image']
                elif isinstance(card['image'], dict):
                    image_url = card['image'].get('large') or card['image'].get('small')
            
            if not image_url:
                print(f"   âš ï¸  [{card_idx}/{len(cards)}] {card_name} ({card_id}): No image URL")
                continue
            
            print(f"   ğŸ”„ [{card_idx}/{len(cards)}] {card_name} ({card_id})...", end=' ')
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ…ĞµÑˆĞ¸
            hashes, error = compute_hashes_for_image(image_url, card_id)
            
            if hashes:
                new_hashes.append({
                    'id': card_id,
                    'perceptual': hashes['perceptual'],
                    'difference': hashes['difference'],
                    'wavelet': hashes['wavelet'],
                    'color': hashes['color']
                })
                total_added += 1
                print("âœ…")
            else:
                print(f"âŒ ({error})")
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸
            if len(new_hashes) >= batch_size:
                print(f"\n   ğŸ’¾ Saving batch of {len(new_hashes)} cards...")
                new_df = pd.DataFrame(new_hashes)
                updated_df = pd.concat([existing_df, new_df], ignore_index=True)
                updated_df.to_pickle('card_hashes_32b.pickle')
                
                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
                existing_df = updated_df
                existing_ids.update([h['id'] for h in new_hashes])
                new_hashes = []
                
                print(f"   âœ… Database now has {len(existing_df)} cards")
                print(f"   ğŸ“Š Progress: {total_added} added, {total_skipped} skipped, {total_processed} processed")
            
            # ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ° Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚ÑŒ API
            time.sleep(0.1)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞµÑ‚Ğ°
        if limit and total_processed >= limit:
            break
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸ĞµÑÑ ĞºĞ°Ñ€Ñ‚Ñ‹
    if new_hashes:
        print(f"\nğŸ’¾ Saving final batch of {len(new_hashes)} cards...")
        new_df = pd.DataFrame(new_hashes)
        updated_df = pd.concat([existing_df, new_df], ignore_index=True)
        updated_df.to_pickle('card_hashes_32b.pickle')
        print(f"âœ… Database now has {len(updated_df)} cards")
    
    # Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² CSV Ğ´Ğ»Ñ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ°
    if os.path.exists('card_hashes_32b.pickle'):
        print("\nğŸ’¾ Saving CSV version...")
        df = pd.read_pickle('card_hashes_32b.pickle')
        
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ…ĞµÑˆĞ¸ Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ´Ğ»Ñ CSV
        df_csv = df.copy()
        df_csv['perceptual'] = df_csv['perceptual'].astype(str)
        df_csv['difference'] = df_csv['difference'].astype(str)
        df_csv['wavelet'] = df_csv['wavelet'].astype(str)
        df_csv['color'] = df_csv['color'].astype(str)
        
        df_csv.to_csv('card_hashes_32b.csv', index=False)
        print(f"âœ… CSV saved with {len(df_csv)} cards")
    
    print("\n" + "="*80)
    print("ğŸ“Š FINAL STATISTICS:")
    print(f"   Total processed: {total_processed} cards")
    print(f"   Newly added: {total_added} cards")
    print(f"   Skipped (already exist): {total_skipped} cards")
    print(f"   Database total: {len(existing_df) + len(new_hashes)} cards")
    print("="*80)
    print("\nâœ… Done!")

if __name__ == '__main__':
    import sys
    
    # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
    batch_size = 50
    start_from = 0
    limit = None
    
    if len(sys.argv) > 1:
        limit = int(sys.argv[1])
        print(f"ğŸ“Œ Processing limit: {limit} cards")
    
    if len(sys.argv) > 2:
        start_from = int(sys.argv[2])
        print(f"ğŸ“Œ Starting from set index: {start_from}")
    
    print("\nğŸ´ TCGdex Hash Database Updater")
    print("="*80)
    print("This script will download card images from TCGdex API")
    print("and compute perceptual hashes for visual matching.")
    print("="*80 + "\n")
    
    try:
        update_hash_database(batch_size=batch_size, start_from=start_from, limit=limit)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user. Progress has been saved.")
        print("You can resume by running the script again with the appropriate start_from parameter.")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
